{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/satyabratkumarsingh/AMLS_23-24_SN-23220011/blob/main/14Oct.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H6ekDbPRZ6wj",
        "outputId": "4e797912-7ded-49f4-8640-822ba83aa716"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.20.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: comet_ml in /usr/local/lib/python3.12/dist-packages (3.53.2)\n",
            "Requirement already satisfied: dulwich!=0.20.33,>=0.20.6 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (0.24.4)\n",
            "Requirement already satisfied: everett<3.2.0,>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from everett[ini]<3.2.0,>=1.0.1->comet_ml) (3.1.0)\n",
            "Requirement already satisfied: jsonschema!=3.1.0,>=2.6.0 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (4.25.1)\n",
            "Requirement already satisfied: psutil>=5.6.3 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (5.9.5)\n",
            "Requirement already satisfied: python-box<7.0.0 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (6.1.0)\n",
            "Requirement already satisfied: requests-toolbelt>=0.8.0 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (1.0.0)\n",
            "Requirement already satisfied: requests>=2.18.4 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (2.32.4)\n",
            "Requirement already satisfied: rich>=13.3.2 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (13.9.4)\n",
            "Requirement already satisfied: semantic-version>=2.8.0 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (2.10.0)\n",
            "Requirement already satisfied: sentry-sdk>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (2.40.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from comet_ml) (75.2.0)\n",
            "Requirement already satisfied: simplejson in /usr/local/lib/python3.12/dist-packages (from comet_ml) (3.20.2)\n",
            "Requirement already satisfied: urllib3>=1.26.8 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (2.5.0)\n",
            "Requirement already satisfied: wrapt>=1.11.2 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (1.17.3)\n",
            "Requirement already satisfied: wurlitzer>=1.0.2 in /usr/local/lib/python3.12/dist-packages (from comet_ml) (3.1.1)\n",
            "Requirement already satisfied: configobj in /usr/local/lib/python3.12/dist-packages (from everett[ini]<3.2.0,>=1.0.1->comet_ml) (5.0.9)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (25.4.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (2025.9.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema!=3.1.0,>=2.6.0->comet_ml) (0.27.1)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.18.4->comet_ml) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.18.4->comet_ml) (3.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.18.4->comet_ml) (2025.10.5)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.3.2->comet_ml) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich>=13.3.2->comet_ml) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich>=13.3.2->comet_ml) (0.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.4.0 in /usr/local/lib/python3.12/dist-packages (from referencing>=0.28.4->jsonschema!=3.1.0,>=2.6.0->comet_ml) (4.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (4.67.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: numpy>=1.23 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install torch\n",
        "!pip install comet_ml\n",
        "!pip install tqdm\n",
        "!pip install matplotlib\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dcuewC3HaGod",
        "outputId": "55afb7bf-2d86-4083-f387-01741826a474"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "VxajueEJaKqx"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "def delete_file_from_drive(full_file_path):\n",
        "  if os.path.exists(full_file_path):\n",
        "      try:\n",
        "          os.remove(full_file_path)\n",
        "          print(f\"File '{full_file_path}' successfully deleted from Google Drive.\")\n",
        "      except Exception as e:\n",
        "          print(f\"Error deleting file '{full_file_path}': {e}\")\n",
        "  else:\n",
        "      print(f\"File '{full_file_path}' not found at '{full_file_path}'.\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sBzJrtMRaTLg",
        "outputId": "0c76d479-e23d-4575-c86a-4661a3dc4a36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r[*********************100%***********************]  1 of 1 completed"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Calibrated SP500 1Y Volatility (Sigma): 0.1950\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "import itertools\n",
        "from itertools import product\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import gc # For garbage collection\n",
        "import numpy as np\n",
        "import yfinance as yf\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def calibrate_sigma_from_sp500(period=\"1y\", ticker=\"SPY\"):\n",
        "    \"\"\"\n",
        "    Calibrate volatility (sigma) using S&P500 proxy (SPY ETF).\n",
        "    Computes realized annualized volatility from historical data.\n",
        "\n",
        "    Args:\n",
        "        period (str): Period to download (e.g. \"1y\", \"2y\").\n",
        "        ticker (str): Symbol to use, default SPY (ETF for S&P 500).\n",
        "\n",
        "    Returns:\n",
        "        float: Annualized volatility (sigma).\n",
        "    \"\"\"\n",
        "    # Download daily adjusted close prices\n",
        "    data = yf.download(ticker, period=period, interval=\"1d\", auto_adjust=True)\n",
        "\n",
        "    closes = data[\"Close\"].dropna()\n",
        "\n",
        "    # Compute log returns\n",
        "    log_returns = np.log(closes / closes.shift(1)).dropna()\n",
        "\n",
        "    # Daily volatility\n",
        "    sigma_daily = log_returns.std()\n",
        "\n",
        "    # Annualized volatility\n",
        "    sigma_annual = sigma_daily * np.sqrt(252)\n",
        "\n",
        "    # Convert safely to float\n",
        "    return sigma_annual.item() if hasattr(sigma_annual, \"item\") else float(sigma_annual)\n",
        "\n",
        "# Example: update SIGMA\n",
        "SIGMA = calibrate_sigma_from_sp500()\n",
        "print(f\"Calibrated SP500 1Y Volatility (Sigma): {SIGMA:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "ydXyqojGaWjs"
      },
      "outputs": [],
      "source": [
        "\n",
        "########### PARAMETERS ###############\n",
        "MU = 0.02\n",
        "T = 1.0\n",
        "NOISE_STD = 0.05\n",
        "MIN_PRICE_RANGE = 100\n",
        "MAX_PRICE_RANGE = 500\n",
        "########################\n",
        "\n",
        "def generate_hybrid_strikes(S_0, option_types, rng, realistic_ratio=0.6):\n",
        "    n = len(option_types)\n",
        "    K_prices = np.zeros(n, dtype=np.float32)\n",
        "\n",
        "    for i in range(n):\n",
        "        if rng.random() < realistic_ratio:\n",
        "            # 60% of data is 'realistic' (Call OTM, Put OTM)\n",
        "            if option_types[i] == \"call\":\n",
        "                # Call OTM: K > S_0\n",
        "                K_prices[i] = S_0 * rng.uniform(1.00, 1.20)\n",
        "            else:\n",
        "                # Put OTM: K < S_0\n",
        "                K_prices[i] = S_0 * rng.uniform(0.80, 1.00)\n",
        "        else:\n",
        "            # 40% of data is 'exploration' (Call ITM, Put ITM)\n",
        "            if option_types[i] == \"call\":\n",
        "                # Call ITM: K < S_0\n",
        "                K_prices[i] = S_0 * rng.uniform(0.80, 1.00)\n",
        "            else:\n",
        "                # Put ITM: K > S_0\n",
        "                K_prices[i] = S_0 * rng.uniform(1.00, 1.20)\n",
        "\n",
        "    return K_prices\n",
        "\n",
        "\n",
        "def _generate_option_types(n, rng, composition_type=\"mixed\", p=(0.5, 0.5)):\n",
        "    if composition_type == \"call_only\":\n",
        "        option_types_string = np.array([\"call\"] * n)\n",
        "        option_types_numeric = np.ones(n, dtype=np.float32)\n",
        "    elif composition_type == \"put_only\":\n",
        "        option_types_string = np.array([\"put\"] * n)\n",
        "        option_types_numeric = -np.ones(n, dtype=np.float32)\n",
        "    else:\n",
        "        option_types_string = rng.choice([\"call\", \"put\"], size=n, p=p)\n",
        "        option_types_numeric = np.where(option_types_string == \"call\", 1.0, -1.0).astype(np.float32)\n",
        "\n",
        "    return option_types_string, option_types_numeric\n",
        "\n",
        "\n",
        "def generate_option_prices_for_idx(seed, n, composition_type=\"mixed\", weights=None, realistic_ratio=0.7):\n",
        "\n",
        "    rng = np.random.default_rng(seed)\n",
        "\n",
        "    torch.manual_seed(seed)\n",
        "    if DEVICE.type == 'cuda':\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "\n",
        "    S_0 = rng.uniform(MIN_PRICE_RANGE, MAX_PRICE_RANGE)\n",
        "\n",
        "    option_types, option_types_numeric = _generate_option_types(n, rng, composition_type)\n",
        "\n",
        "    # Generate hybrid strike prices\n",
        "    K_prices = generate_hybrid_strikes(S_0, option_types, rng, realistic_ratio)\n",
        "\n",
        "    # Generate or use weights\n",
        "    if weights is None:\n",
        "        weight_sets = generate_combinatorial_weights_manageable(n, rng=rng)\n",
        "        weights_array = weight_sets[0]\n",
        "    else:\n",
        "        weights_array = np.array(weights, dtype=np.float32)\n",
        "\n",
        "    return K_prices, option_types_numeric, S_0, weights_array\n",
        "\n",
        "\n",
        "def generate_combinatorial_weights_manageable(\n",
        "    n, base_weights=[-0.75, -0.5, -0.25, 0, 0.25, 0.5, 0.75], rng=None\n",
        "):\n",
        "    if rng is None:\n",
        "        rng = np.random.default_rng()\n",
        "\n",
        "    weight_sets = []\n",
        "\n",
        "    if n < 2:\n",
        "        weights = np.zeros(n, dtype=np.float32)\n",
        "        if n == 1:\n",
        "            weights[0] = 1.0\n",
        "        weight_sets.append(weights)\n",
        "        return weight_sets\n",
        "\n",
        "    weights = np.zeros(n, dtype=np.float32)\n",
        "\n",
        "    # Use rng instead of random\n",
        "    is_long = rng.choice([True, False])\n",
        "\n",
        "    if is_long:\n",
        "        long_idx = rng.integers(0, n)\n",
        "        weights[long_idx] = 1.0\n",
        "    else:\n",
        "        short_idx = rng.integers(0, n)\n",
        "        weights[short_idx] = -1.0\n",
        "\n",
        "    remaining_positions = np.where(weights == 0)[0]\n",
        "    combinatorics = rng.choice(base_weights, size=len(remaining_positions), replace=True)\n",
        "    weights[remaining_positions] = combinatorics\n",
        "\n",
        "    weight_sets.append(weights)\n",
        "    return weight_sets\n",
        "\n",
        "\n",
        "def compute_cashflow_and_delta(portfolio, S_T_batch):\n",
        "\n",
        "    B, N, _ = portfolio.shape\n",
        "    B2, M = S_T_batch.shape\n",
        "    assert B == B2, f\"Batch size mismatch: portfolio {B}, S_T {B2}\"\n",
        "\n",
        "    # Expand tensors for vectorized computation\n",
        "    portfolio_exp = portfolio.unsqueeze(2).expand(-1, -1, M, -1)  # [B, N, M, 3]\n",
        "    S_T_exp = S_T_batch.unsqueeze(1).expand(-1, N, -1)  # [B, N, M]\n",
        "\n",
        "    strikes = portfolio_exp[..., 0]  # [B, N, M]\n",
        "    types = portfolio_exp[..., 1]    # [B, N, M]\n",
        "    weights = portfolio_exp[..., 2]  # [B, N, M]\n",
        "\n",
        "    # Compute payoffs for all scenarios\n",
        "    call_payoffs = torch.relu(S_T_exp - strikes)  # [B, N, M]\n",
        "    put_payoffs = torch.relu(strikes - S_T_exp)   # [B, N, M]\n",
        "    payoffs = torch.where(types == 1, call_payoffs, put_payoffs)\n",
        "\n",
        "    # Compute deltas for all scenarios\n",
        "    call_delta = ((types == 1) & (S_T_exp > strikes)).float()\n",
        "    put_delta = -((types == -1) & (S_T_exp < strikes)).float()\n",
        "    delta_each = call_delta + put_delta  # [B, N, M]\n",
        "\n",
        "    # Aggregate across options (sum over N dimension)\n",
        "    cashflow = (payoffs * weights).sum(dim=1)  # [B, M]\n",
        "    derivative = (delta_each * weights).sum(dim=1)  # [B, M]\n",
        "\n",
        "    return cashflow.float(), derivative.float()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GZ8HcSoSa3dU"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "4W7sGd-Laj4m"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import Dataset\n",
        "\n",
        "class DatasetStandardized(Dataset):\n",
        "\n",
        "    def __init__(self, num_samples, num_samples_S_T, K_scaler=None,\n",
        "                 S_T_scaler=None, cashflow_scaler=None, is_fitting_mode=False,\n",
        "                 max_portfolio_size=100, min_portfolio_size=1, base_seed=42): # Reduced max_portfolio_size to 100 for stability\n",
        "\n",
        "        self.num_samples = num_samples\n",
        "        self.num_samples_S_T = num_samples_S_T\n",
        "        self.max_portfolio_size = max_portfolio_size\n",
        "        self.min_portfolio_size = min_portfolio_size\n",
        "        self.is_fitting_mode = is_fitting_mode\n",
        "\n",
        "        if not is_fitting_mode and any(s is None for s in (K_scaler, S_T_scaler, cashflow_scaler)):\n",
        "            raise ValueError(\"K_scaler, S_T_scaler, and cashflow_scaler must be provided in evaluation mode.\")\n",
        "\n",
        "        self.K_scaler = K_scaler\n",
        "        self.S_T_scaler = S_T_scaler\n",
        "        self.cashflow_scaler = cashflow_scaler\n",
        "        self.base_seed = base_seed\n",
        "        self.epoch = 0\n",
        "\n",
        "    def set_epoch(self, epoch):\n",
        "        self.epoch = epoch\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.num_samples\n",
        "\n",
        "    def _sample_portfolio_len(self):\n",
        "      if self.min_portfolio_size == self.max_portfolio_size:\n",
        "          return self.max_portfolio_size\n",
        "\n",
        "      u = np.random.rand()\n",
        "      low = self.min_portfolio_size\n",
        "      high = self.max_portfolio_size\n",
        "\n",
        "      # --- Special bucket for single-option portfolios (e.g., 10% chance) ---\n",
        "      if low <= 1 and u < 0.10:\n",
        "          return 1\n",
        "\n",
        "      # --- Training mode: stratified distribution with dynamic ranges ---\n",
        "      # Adjust u after the single-option probability\n",
        "      u = (u - 0.10) / 0.90  # rescale remaining probability mass\n",
        "\n",
        "      # Bucket 1: Small Portfolios (up to 10, weighted 35%)\n",
        "      bucket_1_max = min(high, 10)\n",
        "\n",
        "      # Bucket 2: Medium Portfolios (11–50, weighted 20%)\n",
        "      bucket_2_min = max(low, 11)\n",
        "      bucket_2_max = min(high, 50)\n",
        "\n",
        "      # Bucket 3: Large Portfolios (51–200, weighted 20%)\n",
        "      bucket_3_min = max(low, 51)\n",
        "      bucket_3_max = min(high, 200)\n",
        "\n",
        "      # Bucket 4: Very Large Portfolios (201–max, weighted 25%)\n",
        "      bucket_4_min = max(low, 201)\n",
        "      bucket_4_max = high\n",
        "\n",
        "      if u < 0.35 and low <= bucket_1_max:\n",
        "          return np.random.randint(max(low, 2), bucket_1_max + 1)\n",
        "      elif u < 0.55 and bucket_2_min <= bucket_2_max:\n",
        "          return np.random.randint(bucket_2_min, bucket_2_max + 1)\n",
        "      elif u < 0.75 and bucket_3_min <= bucket_3_max:\n",
        "          return np.random.randint(bucket_3_min, bucket_3_max + 1)\n",
        "      elif bucket_4_min <= bucket_4_max:\n",
        "          return np.random.randint(bucket_4_min, bucket_4_max + 1)\n",
        "      else:\n",
        "          return np.random.randint(low, high + 1)\n",
        "\n",
        "\n",
        "\n",
        "    def _simulate_terminal_prices(self, S_0, seed=None):\n",
        "        g = torch.Generator()\n",
        "        if seed is not None:\n",
        "            g.manual_seed(seed)\n",
        "\n",
        "        Z = torch.clamp(torch.randn(self.num_samples_S_T, generator=g), -3, 3)\n",
        "        drift = (MU - 0.5 * SIGMA**2) * T\n",
        "        diffusion = SIGMA * torch.sqrt(torch.tensor(T, dtype=torch.float32))\n",
        "\n",
        "        S_T = S_0 * torch.exp(drift + diffusion * Z)\n",
        "\n",
        "        noise = torch.randn(S_T.shape, generator=g, dtype=S_T.dtype, device=S_T.device) * (NOISE_STD * S_T)\n",
        "        S_T += noise\n",
        "\n",
        "        return S_T.float()\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "\n",
        "        portfolio_len = self._sample_portfolio_len()\n",
        "        unique_seed = self.base_seed + self.epoch * 1_000_000 + idx\n",
        "\n",
        "        np.random.seed(unique_seed)\n",
        "        torch.manual_seed(unique_seed)\n",
        "\n",
        "        # Put all types as equal probablities\n",
        "        composition_choices = [\"call_only\", \"put_only\", \"mixed\"]\n",
        "        probabilities = [0.35, 0.35, 0.30]\n",
        "        composition_type = np.random.choice(composition_choices, p=probabilities)\n",
        "\n",
        "\n",
        "        K, option_types, S_0, weights = generate_option_prices_for_idx(unique_seed, portfolio_len, composition_type)\n",
        "\n",
        "        # Convert to unpadded PyTorch tensors\n",
        "        K = torch.tensor(K, dtype=torch.float32)\n",
        "        option_types = torch.tensor(option_types, dtype=torch.float32)\n",
        "        weights = torch.tensor(weights, dtype=torch.float32)\n",
        "        S_0 = torch.tensor(S_0, dtype=torch.float32)\n",
        "\n",
        "\n",
        "        S_0_Broadcast = S_0.repeat(len(K)) if S_0.ndim == 0 else S_0\n",
        "\n",
        "        # Unpadded Denormalized Portfolio\n",
        "        portfolio_denorm_raw = torch.stack([K, option_types, weights, S_0_Broadcast], dim=-1)\n",
        "\n",
        "        # Simulate terminal prices\n",
        "        S_T_denorm = self._simulate_terminal_prices(S_0, seed=unique_seed)\n",
        "\n",
        "        # Cashflows + derivatives (denormalized, unpadded)\n",
        "        cashflow_denorm, derivative_denorm = compute_cashflow_and_delta(\n",
        "            portfolio_denorm_raw.unsqueeze(0), S_T_denorm.unsqueeze(0)\n",
        "        )\n",
        "        cashflow_denorm = cashflow_denorm.squeeze(0).float()\n",
        "        derivative_denorm = derivative_denorm.squeeze(0).float()\n",
        "\n",
        "        # === Normalization and Scaling ===\n",
        "        if not self.is_fitting_mode:\n",
        "            # 1. Normalize K using .numpy()\n",
        "            K_norm = torch.tensor(\n",
        "                self.K_scaler.transform(K.unsqueeze(1).numpy()), dtype=torch.float32\n",
        "            ).squeeze()\n",
        "\n",
        "            # 2. Create the normalized, unpadded portfolio\n",
        "            portfolio_norm_raw = portfolio_denorm_raw.clone()\n",
        "            portfolio_norm_raw[:, 0] = K_norm\n",
        "\n",
        "            # 3. Normalize S_T using .numpy()\n",
        "            S_T = torch.tensor(\n",
        "                self.S_T_scaler.transform(S_T_denorm.unsqueeze(1).numpy()),\n",
        "                dtype=torch.float32\n",
        "            ).squeeze()\n",
        "\n",
        "            # 4. Normalize cashflow using .numpy()\n",
        "            cashflow = torch.tensor(\n",
        "                self.cashflow_scaler.transform(cashflow_denorm.unsqueeze(1).numpy()),\n",
        "                dtype=torch.float32\n",
        "            ).squeeze()\n",
        "\n",
        "            # 5. Scale derivative\n",
        "            derivative = derivative_denorm.clone()\n",
        "            cf_std = cashflow.std().item()\n",
        "            deriv_std = derivative.std().item()\n",
        "            if deriv_std > 0:\n",
        "                derivative *= (cf_std / deriv_std)\n",
        "\n",
        "        else:\n",
        "            # In fitting mode, use denormalized values\n",
        "            portfolio_norm_raw = portfolio_denorm_raw\n",
        "            S_T = S_T_denorm\n",
        "            cashflow = cashflow_denorm\n",
        "            derivative = derivative_denorm\n",
        "\n",
        "        pad_len = self.max_portfolio_size - portfolio_len\n",
        "\n",
        "        if pad_len > 0:\n",
        "            # ✅ Fixed padding dimension (was 3 before, now correctly 4)\n",
        "            pad_tensor = torch.zeros(pad_len, 4, dtype=torch.float32)\n",
        "\n",
        "            # Pad the normalized portfolio (final input)\n",
        "            portfolio = torch.cat([portfolio_norm_raw, pad_tensor], dim=0)\n",
        "\n",
        "            # Pad the denormalized portfolio (for checks)\n",
        "            portfolio_denorm = torch.cat([portfolio_denorm_raw, pad_tensor], dim=0)\n",
        "\n",
        "            # Create the final mask (must be max_portfolio_size long)\n",
        "            mask = torch.tensor([True]*portfolio_len + [False]*pad_len, dtype=torch.bool)\n",
        "        else:\n",
        "            portfolio = portfolio_norm_raw\n",
        "            portfolio_denorm = portfolio_denorm_raw\n",
        "            mask = torch.ones(portfolio_len, dtype=torch.bool)\n",
        "\n",
        "        return {\n",
        "            # Normalized (for training)\n",
        "            \"portfolio\": portfolio.float(),          # [max_portfolio_size, 4]\n",
        "            \"mask\": mask,                            # [max_portfolio_size]\n",
        "            \"S_T\": S_T.float(),                      # [num_samples_S_T]\n",
        "            \"cashflow\": cashflow.float(),            # [num_samples_S_T]\n",
        "            \"derivative\": derivative.float(),        # [num_samples_S_T]\n",
        "\n",
        "            # Denormalized (for evaluation / interpretation)\n",
        "            \"portfolio_denorm\": portfolio_denorm.float(),\n",
        "            \"S_T_denorm\": S_T_denorm.float(),\n",
        "            \"cashflow_denorm\": cashflow_denorm.float(),\n",
        "            \"derivative_denorm\": derivative_denorm.float(),\n",
        "        }"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zWJVnXDTa548",
        "outputId": "5cf4929b-210a-4a5c-91bd-2f3354fa319e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Sample 0 ===\n",
            "Portfolio features:\n",
            " tensor([[397.9993,  -1.0000,   0.0000, 409.5824],\n",
            "        [335.3806,  -1.0000,  -0.7500, 409.5824],\n",
            "        [471.9323,  -1.0000,  -1.0000, 409.5824],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000]])\n",
            "Mask: tensor([ True,  True,  True, False, False])\n",
            "S_T: tensor([442.9435, 425.2559])\n",
            "Cashflow: tensor([-28.9888, -46.6764])\n",
            "Derivative: tensor([1., 1.])\n",
            "Portfolio (denorm):\n",
            " tensor([[397.9993,  -1.0000,   0.0000, 409.5824],\n",
            "        [335.3806,  -1.0000,  -0.7500, 409.5824],\n",
            "        [471.9323,  -1.0000,  -1.0000, 409.5824],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000]])\n",
            "S_T (denorm): tensor([442.9435, 425.2559])\n",
            "Cashflow (denorm): tensor([-28.9888, -46.6764])\n",
            "Derivative (denorm): tensor([1., 1.])\n",
            "\n",
            "=== Sample 1 ===\n",
            "Portfolio features:\n",
            " tensor([[ 3.6237e+02,  1.0000e+00,  1.0000e+00,  3.6092e+02],\n",
            "        [ 3.3112e+02,  1.0000e+00, -5.0000e-01,  3.6092e+02],\n",
            "        [ 4.1519e+02,  1.0000e+00, -2.5000e-01,  3.6092e+02],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "Mask: tensor([ True,  True,  True, False, False])\n",
            "S_T: tensor([328.5923, 338.0837])\n",
            "Cashflow: tensor([ 0.0000, -3.4828])\n",
            "Derivative: tensor([ 0.0000, -0.5000])\n",
            "Portfolio (denorm):\n",
            " tensor([[ 3.6237e+02,  1.0000e+00,  1.0000e+00,  3.6092e+02],\n",
            "        [ 3.3112e+02,  1.0000e+00, -5.0000e-01,  3.6092e+02],\n",
            "        [ 4.1519e+02,  1.0000e+00, -2.5000e-01,  3.6092e+02],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00],\n",
            "        [ 0.0000e+00,  0.0000e+00,  0.0000e+00,  0.0000e+00]])\n",
            "S_T (denorm): tensor([328.5923, 338.0837])\n",
            "Cashflow (denorm): tensor([ 0.0000, -3.4828])\n",
            "Derivative (denorm): tensor([ 0.0000, -0.5000])\n",
            "\n",
            "=== Sample 2 ===\n",
            "Portfolio features:\n",
            " tensor([[174.5780,   1.0000,  -0.2500, 149.0262],\n",
            "        [159.0992,   1.0000,  -0.2500, 149.0262],\n",
            "        [137.5969,  -1.0000,   1.0000, 149.0262],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000]])\n",
            "Mask: tensor([ True,  True,  True, False, False])\n",
            "S_T: tensor([157.2707, 114.3691])\n",
            "Cashflow: tensor([ 0.0000, 23.2278])\n",
            "Derivative: tensor([ 0., -1.])\n",
            "Portfolio (denorm):\n",
            " tensor([[174.5780,   1.0000,  -0.2500, 149.0262],\n",
            "        [159.0992,   1.0000,  -0.2500, 149.0262],\n",
            "        [137.5969,  -1.0000,   1.0000, 149.0262],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000]])\n",
            "S_T (denorm): tensor([157.2707, 114.3691])\n",
            "Cashflow (denorm): tensor([ 0.0000, 23.2278])\n",
            "Derivative (denorm): tensor([ 0., -1.])\n",
            "\n",
            "=== Sample 3 ===\n",
            "Portfolio features:\n",
            " tensor([[ 3.6842e+02, -1.0000e+00,  5.0000e-01,  3.2925e+02],\n",
            "        [ 3.0761e+02, -1.0000e+00,  7.5000e-01,  3.2925e+02],\n",
            "        [ 3.1872e+02, -1.0000e+00, -2.5000e-01,  3.2925e+02],\n",
            "        [ 3.6409e+02, -1.0000e+00,  2.5000e-01,  3.2925e+02],\n",
            "        [ 3.6012e+02, -1.0000e+00,  1.0000e+00,  3.2925e+02]])\n",
            "Mask: tensor([True, True, True, True, True])\n",
            "S_T: tensor([255.7679, 339.3480])\n",
            "Cashflow: tensor([210.8966,  41.4916])\n",
            "Derivative: tensor([-2.2500, -1.7500])\n",
            "Portfolio (denorm):\n",
            " tensor([[ 3.6842e+02, -1.0000e+00,  5.0000e-01,  3.2925e+02],\n",
            "        [ 3.0761e+02, -1.0000e+00,  7.5000e-01,  3.2925e+02],\n",
            "        [ 3.1872e+02, -1.0000e+00, -2.5000e-01,  3.2925e+02],\n",
            "        [ 3.6409e+02, -1.0000e+00,  2.5000e-01,  3.2925e+02],\n",
            "        [ 3.6012e+02, -1.0000e+00,  1.0000e+00,  3.2925e+02]])\n",
            "S_T (denorm): tensor([255.7679, 339.3480])\n",
            "Cashflow (denorm): tensor([210.8966,  41.4916])\n",
            "Derivative (denorm): tensor([-2.2500, -1.7500])\n",
            "\n",
            "=== Sample 4 ===\n",
            "Portfolio features:\n",
            " tensor([[519.7308,   1.0000,   1.0000, 462.2417],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000]])\n",
            "Mask: tensor([ True, False, False, False, False])\n",
            "S_T: tensor([478.1302, 521.2719])\n",
            "Cashflow: tensor([0.0000, 1.5411])\n",
            "Derivative: tensor([0., 1.])\n",
            "Portfolio (denorm):\n",
            " tensor([[519.7308,   1.0000,   1.0000, 462.2417],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000],\n",
            "        [  0.0000,   0.0000,   0.0000,   0.0000]])\n",
            "S_T (denorm): tensor([478.1302, 521.2719])\n",
            "Cashflow (denorm): tensor([0.0000, 1.5411])\n",
            "Derivative (denorm): tensor([0., 1.])\n"
          ]
        }
      ],
      "source": [
        "# --- Quick test run ---\n",
        "dataset = DatasetStandardized(\n",
        "    num_samples=5,\n",
        "    num_samples_S_T=2,\n",
        "    K_scaler=None, S_T_scaler=None, cashflow_scaler=None,\n",
        "    max_portfolio_size=5,\n",
        "    min_portfolio_size=1,\n",
        "    is_fitting_mode=True\n",
        ")\n",
        "\n",
        "for i in range(len(dataset)):\n",
        "    sample = dataset[i]  # dictionary returned\n",
        "    print(f\"\\n=== Sample {i} ===\")\n",
        "    print(\"Portfolio features:\\n\", sample[\"portfolio\"])\n",
        "    print(\"Mask:\", sample[\"mask\"])\n",
        "    print(\"S_T:\", sample[\"S_T\"])\n",
        "    print(\"Cashflow:\", sample[\"cashflow\"])\n",
        "    print(\"Derivative:\", sample[\"derivative\"])\n",
        "    print(\"Portfolio (denorm):\\n\", sample[\"portfolio_denorm\"])\n",
        "    print(\"S_T (denorm):\", sample[\"S_T_denorm\"])\n",
        "    print(\"Cashflow (denorm):\", sample[\"cashflow_denorm\"])\n",
        "    print(\"Derivative (denorm):\", sample[\"derivative_denorm\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LfiU8hu3gQmj"
      },
      "source": [
        "# Utility Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "EjEmIoLEewi4"
      },
      "outputs": [],
      "source": [
        "\n",
        "def collate_fn(batch):\n",
        "    max_len = max(item[\"mask\"].shape[0] for item in batch)\n",
        "    keys_to_stack = [\"S_T\", \"S_T_denorm\", \"cashflow\", \"cashflow_denorm\", \"derivative\", \"derivative_denorm\"]\n",
        "\n",
        "    def pad_tensor(tensor, target_len):\n",
        "        if tensor.shape[0] < target_len:\n",
        "            pad = torch.zeros(target_len - tensor.shape[0], tensor.shape[-1])\n",
        "            tensor = torch.cat([tensor, pad], dim=0)\n",
        "        return tensor\n",
        "\n",
        "    # Pad portfolios (normalized + denormalized)\n",
        "    portfolios = torch.stack([pad_tensor(item[\"portfolio\"], max_len) for item in batch])\n",
        "    portfolios_denorm = torch.stack([pad_tensor(item[\"portfolio_denorm\"], max_len) for item in batch])\n",
        "\n",
        "    # Pad masks\n",
        "    masks = torch.stack([\n",
        "        torch.cat([item[\"mask\"], torch.zeros(max_len - item[\"mask\"].shape[0], dtype=torch.bool)])\n",
        "        for item in batch\n",
        "    ])\n",
        "\n",
        "    # Stack everything else directly (no padding needed)\n",
        "    collated = {\n",
        "        \"portfolio\": portfolios,\n",
        "        \"portfolio_denorm\": portfolios_denorm,\n",
        "        \"mask\": masks\n",
        "    }\n",
        "    for key in keys_to_stack:\n",
        "        collated[key] = torch.stack([item[key] for item in batch])\n",
        "\n",
        "    return collated\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "sI3CEz9PgUAW"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import joblib\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/Ucl/\"\n",
        "K_SCALAR_FILE = os.path.join(DRIVE_PATH, 'K_Scalar_Advanced.pkl')\n",
        "ST_SCALAR_FILE = os.path.join(DRIVE_PATH, 'S_T_Scalar_Advanced.pkl')\n",
        "CASHFLOW_SCALAR_FILE = os.path.join(DRIVE_PATH, 'Cashflow_Scalar_Advanced.pkl')\n",
        "\n",
        "\n",
        "def fit_K_ST_scalers(train_loader, save_path_K=K_SCALAR_FILE, save_path_ST=ST_SCALAR_FILE):\n",
        "    print(\"Fitting K and S_T scalers from training set...\")\n",
        "    all_K = []\n",
        "    all_S_T = []\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=\"Collecting K and S_T for scalers\"):\n",
        "\n",
        "        portfolio_real = batch[\"portfolio_denorm\"]  # [B, N, 3]\n",
        "        s_t_real = batch[\"S_T_denorm\"]              # [B, num_S_T]\n",
        "\n",
        "        K_real = portfolio_real[:, :, 0].cpu().numpy().reshape(-1, 1)\n",
        "        S_T_real = s_t_real.cpu().numpy().reshape(-1, 1)\n",
        "\n",
        "        all_K.append(K_real)\n",
        "        all_S_T.append(S_T_real)\n",
        "\n",
        "    K_all_np = np.concatenate(all_K, axis=0)\n",
        "    S_T_all_np = np.concatenate(all_S_T, axis=0)\n",
        "\n",
        "    K_scalar = StandardScaler().fit(K_all_np)\n",
        "    S_T_scalar = StandardScaler().fit(S_T_all_np)\n",
        "\n",
        "    # joblib.dump(K_scalar, save_path_K)\n",
        "    # joblib.dump(S_T_scalar, save_path_ST)\n",
        "\n",
        "    print(f\"K mean: {K_scalar.mean_[0]:.4f}, std: {K_scalar.scale_[0]:.4f}\")\n",
        "    print(f\"S_T mean: {S_T_scalar.mean_[0]:.4f}, std: {S_T_scalar.scale_[0]:.4f}\")\n",
        "\n",
        "    return K_scalar, S_T_scalar\n",
        "\n",
        "\n",
        "\n",
        "def fit_cashflow_scaler(train_loader, save_path=CASHFLOW_SCALAR_FILE):\n",
        "    all_cashflows = []\n",
        "\n",
        "    for batch in tqdm(train_loader, desc=\"Fitting Cashflow Scaler\"):\n",
        "        cashflow = batch[\"cashflow_denorm\"]  # use denormalized\n",
        "        cashflow_np = cashflow.detach().cpu().numpy().reshape(-1, 1)\n",
        "        all_cashflows.append(cashflow_np)\n",
        "\n",
        "    cashflows_np = np.concatenate(all_cashflows, axis=0)\n",
        "\n",
        "    scaler = StandardScaler().fit(cashflows_np)\n",
        "\n",
        "    print(f\"Cashflow Mean: {scaler.mean_[0]:.4f}, Std Dev: {scaler.scale_[0]:.4f}\")\n",
        "\n",
        "    return scaler\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQEnG2Amgrml"
      },
      "source": [
        "# The Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "Tt7bHCAMgX6S"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import math\n",
        "\n",
        "DRPO_OUT_PROB = 0.4\n",
        "NUM_SEEDS = 32\n",
        "NUM_HEADS = 8\n",
        "\n",
        "# ===================== Trunk Network =====================\n",
        "class TrunkNet(nn.Module):\n",
        "    def __init__(self, input_dim=1, latent_dim=256, hidden_dim=64, num_layers=6):\n",
        "        super().__init__()\n",
        "        self.input_proj = nn.Sequential(\n",
        "            nn.Linear(input_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.ReLU()\n",
        "        )\n",
        "        self.blocks = nn.ModuleList([\n",
        "            nn.Sequential(\n",
        "                nn.Linear(hidden_dim, hidden_dim),\n",
        "                nn.LayerNorm(hidden_dim),\n",
        "                nn.ReLU()\n",
        "            ) for _ in range(num_layers)\n",
        "        ])\n",
        "        self.output_proj = nn.Linear(hidden_dim, latent_dim)\n",
        "\n",
        "    def forward(self, S_T):\n",
        "        if S_T.dim() == 1:\n",
        "            S_T = S_T.unsqueeze(-1)\n",
        "        elif S_T.dim() == 2:\n",
        "            S_T = S_T.unsqueeze(-1)\n",
        "\n",
        "        x = self.input_proj(S_T)\n",
        "        for block in self.blocks:\n",
        "            x = x + block(x)\n",
        "        return self.output_proj(x)\n",
        "\n",
        "class PMA(nn.Module):\n",
        "\n",
        "    def __init__(self, d_model, num_heads, num_seeds=NUM_SEEDS, dropout=DRPO_OUT_PROB):\n",
        "        super().__init__()\n",
        "        self.num_seeds = num_seeds\n",
        "        self.d_model = d_model\n",
        "\n",
        "        # Initialize seeds with smaller values for stability\n",
        "        self.seed_vectors = nn.Parameter(torch.randn(num_seeds, d_model) * 0.02)\n",
        "\n",
        "        self.attention = nn.MultiheadAttention(\n",
        "            d_model, num_heads, dropout=dropout, batch_first=True\n",
        "        )\n",
        "\n",
        "        self.norm1 = nn.LayerNorm(d_model)\n",
        "        self.norm2 = nn.LayerNorm(d_model)\n",
        "        self.ffn = nn.Sequential(\n",
        "            nn.Linear(d_model, d_model * 3),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(d_model * 3, d_model),\n",
        "            nn.Dropout(dropout)\n",
        "        )\n",
        "\n",
        "        # Scaling factor for attention (prevents saturation with large sets)\n",
        "        self.scale = nn.Parameter(torch.ones(1))\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        B, N, _ = x.shape\n",
        "        S = self.seed_vectors.unsqueeze(0).expand(B, -1, -1)\n",
        "\n",
        "        key_padding_mask = None\n",
        "        if mask is not None:\n",
        "            key_padding_mask = (~mask.bool())\n",
        "            N_effective = mask.sum(dim=1, keepdim=True).float().clamp(min=1).unsqueeze(-1)\n",
        "\n",
        "        # Apply scaled attention\n",
        "        attn_out, attn_weights = self.attention(\n",
        "            query=S,\n",
        "            key=x,\n",
        "            value=x,\n",
        "            key_padding_mask=key_padding_mask\n",
        "        )\n",
        "        if N_effective is not None:\n",
        "            attn_out = attn_out / N_effective\n",
        "\n",
        "        attn_out = attn_out * self.scale\n",
        "\n",
        "        S = self.norm1(S + attn_out)\n",
        "        S = self.norm2(S + self.ffn(S))\n",
        "        return S\n",
        "\n",
        "\n",
        "# ===================== DeepSets Encoder =====================\n",
        "class DeepSetEncoder(nn.Module):\n",
        "    \"\"\"DeepSets with multiple aggregation strategies for robustness.\"\"\"\n",
        "    def __init__(self, feature_dim, hidden_dim, dropout=DRPO_OUT_PROB):\n",
        "        super().__init__()\n",
        "        # Deeper encoder for better feature extraction\n",
        "        self.encoder = nn.Sequential(\n",
        "            nn.Linear(feature_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.LayerNorm(hidden_dim),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout),\n",
        "            nn.Linear(hidden_dim, hidden_dim),\n",
        "            nn.GELU()\n",
        "        )\n",
        "        self.hidden_dim = hidden_dim\n",
        "\n",
        "    def forward(self, x, mask=None):\n",
        "        H = self.encoder(x)  # [B, N, hidden_dim]\n",
        "\n",
        "        LARGE_NEG = -1e9\n",
        "        LARGE_POS = 1e9\n",
        "\n",
        "        if mask is not None:\n",
        "            mask_f = mask.float().unsqueeze(-1)\n",
        "\n",
        "            # Max/Min aggregation\n",
        "            max_H, _ = (H + LARGE_NEG * (1 - mask_f)).max(dim=1)\n",
        "            min_H, _ = (H + LARGE_POS * (1 - mask_f)).min(dim=1)\n",
        "\n",
        "            # Mean aggregation (size-normalized by definition)\n",
        "            sum_H = (H * mask_f).sum(dim=1)\n",
        "            count = mask_f.sum(dim=1).clamp(min=1)\n",
        "            mean_H = sum_H / count\n",
        "\n",
        "            # Std aggregation (normalized) - handle single element case\n",
        "            count_for_std = count.clamp(min=2)  # Need at least 2 for std\n",
        "            var_H = ((H - mean_H.unsqueeze(1))**2 * mask_f).sum(dim=1) / (count_for_std - 1)\n",
        "            std_H = torch.sqrt(var_H.clamp(min=0) + 1e-8)\n",
        "\n",
        "            # For single-element sets, std should be zero\n",
        "            std_H = torch.where(count < 2, torch.zeros_like(std_H), std_H)\n",
        "        else:\n",
        "            max_H, _ = H.max(dim=1)\n",
        "            min_H, _ = H.min(dim=1)\n",
        "            mean_H = H.mean(dim=1)\n",
        "\n",
        "            # Handle std for potentially single-element batches\n",
        "            if H.size(1) == 1:\n",
        "                std_H = torch.zeros_like(mean_H)\n",
        "            else:\n",
        "                std_H = H.std(dim=1, unbiased=True)\n",
        "\n",
        "        # Combine multiple statistics (all size-invariant)\n",
        "        h_aggregated = torch.cat([mean_H, std_H, max_H, min_H], dim=-1)\n",
        "        return h_aggregated  # [B, 4 * hidden_dim]\n",
        "\n",
        "\n",
        "# ===================== Size-Invariant Hybrid Branch =====================\n",
        "class BranchNet(nn.Module):\n",
        "    \"\"\"Truly size-invariant hybrid encoder using normalized aggregations.\"\"\"\n",
        "    def __init__(self, portfolio_feature_dim, hidden_dim=128, latent_dim=256,\n",
        "                 dropout_prob=DRPO_OUT_PROB, num_heads=NUM_HEADS, num_seeds=NUM_SEEDS,\n",
        "                 use_pma=True):\n",
        "        super().__init__()\n",
        "        self.use_pma = use_pma\n",
        "\n",
        "        self.deepsets_module = DeepSetEncoder(\n",
        "            portfolio_feature_dim, hidden_dim, dropout_prob\n",
        "        )\n",
        "        self.effective_hidden_dim = hidden_dim * 4\n",
        "        self.layer_norm_fusion = nn.LayerNorm(self.effective_hidden_dim)\n",
        "\n",
        "        if use_pma:\n",
        "            self.pma = PMA(hidden_dim, num_heads, num_seeds, dropout_prob)\n",
        "\n",
        "            self.pma_output_proj = nn.Sequential(\n",
        "                nn.Linear(hidden_dim * num_seeds, self.effective_hidden_dim),\n",
        "                nn.LayerNorm(self.effective_hidden_dim),\n",
        "                nn.GELU(),\n",
        "                nn.Dropout(dropout_prob)\n",
        "            )\n",
        "\n",
        "            self.alpha = nn.Parameter(torch.full((self.effective_hidden_dim,), 0.5))\n",
        "\n",
        "        self.output_proj = nn.Sequential(\n",
        "            nn.LayerNorm(self.effective_hidden_dim),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(self.effective_hidden_dim, hidden_dim * 2),\n",
        "            nn.GELU(),\n",
        "            nn.Dropout(dropout_prob),\n",
        "            nn.Linear(hidden_dim * 2, latent_dim)\n",
        "        )\n",
        "\n",
        "    def forward(self, portfolio, mask=None):\n",
        "        # DeepSets path: size-invariant by design (mean, std, max, min)\n",
        "        h_deepsets = self.deepsets_module(portfolio, mask)  # [B, 4*H]\n",
        "\n",
        "        if self.use_pma:\n",
        "\n",
        "            h_encoded_per_item = self.deepsets_module.encoder(portfolio)  # [B, N, H]\n",
        "            h_pma_seeds = self.pma(h_encoded_per_item, mask)  # [B, num_seeds, H]\n",
        "\n",
        "            # Flatten seeds\n",
        "            h_pma_flat = h_pma_seeds.flatten(1)  # [B, num_seeds * H]\n",
        "            h_pma_proj = self.pma_output_proj(h_pma_flat)  # [B, 4*H]\n",
        "\n",
        "            # Simple learnable weighted fusion\n",
        "            alpha = torch.sigmoid(self.alpha)\n",
        "            h_fused = alpha * h_deepsets + (1 - alpha) * h_pma_proj\n",
        "        else:\n",
        "            h_fused = h_deepsets\n",
        "        h_fused = self.layer_norm_fusion(h_fused)\n",
        "        return self.output_proj(h_fused)\n",
        "\n",
        "\n",
        "# ===================== Final DeepONet =====================\n",
        "class DeepONet(nn.Module):\n",
        "    \"\"\"DeepONet with truly size-invariant set encoding.\"\"\"\n",
        "    def __init__(self, portfolio_feature_dim=3, hidden_dim=64, latent_dim=256,\n",
        "                 dropout_prob=DRPO_OUT_PROB, num_heads=NUM_HEADS, use_pma=True,\n",
        "                 num_seeds=NUM_SEEDS):\n",
        "        super().__init__()\n",
        "\n",
        "        self.branch_net = BranchNet(\n",
        "            portfolio_feature_dim=portfolio_feature_dim,\n",
        "            hidden_dim=hidden_dim,\n",
        "            latent_dim=latent_dim,\n",
        "            dropout_prob=dropout_prob,\n",
        "            num_heads=num_heads,\n",
        "            num_seeds=num_seeds,\n",
        "            use_pma=use_pma\n",
        "        )\n",
        "\n",
        "        self.trunk_net = TrunkNet(\n",
        "            input_dim=1,\n",
        "            latent_dim=latent_dim,\n",
        "            hidden_dim=hidden_dim\n",
        "        )\n",
        "\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "        self.branch_scale = nn.Parameter(torch.ones(1))\n",
        "        self.trunk_scale = nn.Parameter(torch.ones(1))\n",
        "\n",
        "    def forward(self, portfolio, S_T, mask=None):\n",
        "        \"\"\"\n",
        "        portfolio: [B, N, feature_dim]\n",
        "        S_T: [B, M]\n",
        "        mask: [B, N]\n",
        "        \"\"\"\n",
        "        branch_out = self.branch_net(portfolio, mask=mask) * self.branch_scale\n",
        "        trunk_out = self.trunk_net(S_T.unsqueeze(-1)) * self.trunk_scale\n",
        "\n",
        "        branch_expanded = branch_out.unsqueeze(1)\n",
        "        interaction = (branch_expanded * trunk_out).sum(dim=-1)\n",
        "\n",
        "        return interaction + self.bias"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aIlvE-vKgmUw"
      },
      "source": [
        "# Gradient Prints"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "HubzTVJ1gb-O"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.amp import GradScaler\n",
        "\n",
        "# --- Gradient summary helper ---\n",
        "def compute_gradient_stats(model):\n",
        "    gradient_stats = {}\n",
        "    total_norm = 0.0\n",
        "    for name, param in model.named_parameters():\n",
        "        if param.grad is not None:\n",
        "            grad_norm = param.grad.norm().item()\n",
        "            total_norm += grad_norm ** 2\n",
        "            gradient_stats[name] = {\n",
        "                'norm': grad_norm,\n",
        "                'shape': tuple(param.grad.shape),\n",
        "                'numel': param.grad.numel(),\n",
        "                'mean': param.grad.mean().item(),\n",
        "                'std': param.grad.std().item()\n",
        "            }\n",
        "    total_norm = total_norm ** 0.5\n",
        "    return total_norm, gradient_stats, None\n",
        "\n",
        "def print_gradient_summary(gradient_stats, total_norm, epoch, batch_idx=None):\n",
        "    prefix = f\"Epoch {epoch}\" + (f\", Batch {batch_idx}\" if batch_idx is not None else \"\")\n",
        "    print(f\"\\n🔍 === Gradient Analysis - {prefix} ===\")\n",
        "    print(f\"Total Gradient Norm: {total_norm:.6f}\")\n",
        "\n",
        "    if total_norm > 30.0:\n",
        "        print(\"🚨 CRITICAL: Severe gradient explosion! Consider stopping training.\")\n",
        "    elif total_norm > 20.0:\n",
        "        print(\"⚠️  SEVERE: Major gradient explosion detected!\")\n",
        "    elif total_norm > 10.0:\n",
        "        print(\"⚠️  WARNING: Moderate gradient explosion detected!\")\n",
        "    elif total_norm < 1e-6:\n",
        "        print(\"⚠️  WARNING: Vanishing gradients detected!\")\n",
        "    else:\n",
        "        print(\"✅ Gradient norm is healthy\")\n",
        "\n",
        "    sorted_layers = sorted(gradient_stats.items(), key=lambda x: x[1]['norm'], reverse=True)\n",
        "    print(f\"\\nTop 5 layers by gradient norm (out of {len(gradient_stats)} total):\")\n",
        "    for i, (layer_name, stats) in enumerate(sorted_layers[:5]):\n",
        "        status = \"🔥\" if stats['norm'] > 3.0 else \"⚠️\" if stats['norm'] > 1.0 else \"✅\"\n",
        "        print(f\"  {status} {i+1}. {layer_name}: {stats['norm']:.4f}\")\n",
        "        print(f\"      Shape: {stats['shape']}, Elements: {stats['numel']}\")\n",
        "        print(f\"      Mean: {stats['mean']:.6f}, Std: {stats['std']:.6f}\")\n",
        "    print(\"=\" * 60)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S17wHxIjghDk"
      },
      "source": [
        "# Early Stopping\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "rRcDrrhMgeJy"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
        "\n",
        "class ExtendedEarlyStopping:\n",
        "    # ... (no changes needed here) ...\n",
        "    def __init__(self, patience=30, min_delta=0.0005, restore_best_weights=True):\n",
        "        self.patience = patience\n",
        "        self.min_delta = min_delta\n",
        "        self.restore_best_weights = restore_best_weights\n",
        "        self.wait = 0\n",
        "        self.stopped_epoch = 0\n",
        "        self.best = float('inf')\n",
        "        self.best_weights = None\n",
        "\n",
        "    def __call__(self, val_loss, model=None):\n",
        "        if val_loss < self.best - self.min_delta:\n",
        "            self.best = val_loss\n",
        "            self.wait = 0\n",
        "            if model is not None and self.restore_best_weights:\n",
        "                self.best_weights = model.state_dict().copy()\n",
        "        else:\n",
        "            self.wait += 1\n",
        "\n",
        "        if self.wait >= self.patience:\n",
        "            self.stopped_epoch = True\n",
        "            if model is not None and self.restore_best_weights and self.best_weights is not None:\n",
        "                model.load_state_dict(self.best_weights)\n",
        "\n",
        "        return self.stopped_epoch\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uuxk-N2hg0Ty"
      },
      "source": [
        "# The Trainer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "qEHNzT75ggfZ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.cuda.amp import GradScaler # Assuming GradScaler is used for mixed precision\n",
        "\n",
        "# Trainer\n",
        "class OptimizedTrainer:\n",
        "    def __init__(self, model, device='cuda', monitor_gradients=True,\n",
        "                 learning_rate=5e-6, lambda_deriv_weight=0.1, weight_decay=1e-4,\n",
        "                 scheduler_T_0=10, scheduler_T_mult=2, scheduler_eta_min=1e-6, # New scheduler parameters integrated\n",
        "                 warmup_epochs=5, initial_scale=0.05, final_scale=1.0, grad_log_threshold = 5.0):\n",
        "        self.model = model.to(device)\n",
        "        self.device = device\n",
        "        self.monitor_gradients = monitor_gradients\n",
        "        self.lambda_deriv_weight = lambda_deriv_weight\n",
        "\n",
        "        # Optimizer\n",
        "        self.optimizer = optim.AdamW(\n",
        "            model.parameters(),\n",
        "            lr=learning_rate,\n",
        "            weight_decay=weight_decay,\n",
        "            betas=(0.9, 0.999),\n",
        "            eps=1e-8\n",
        "        )\n",
        "\n",
        "        # Cosine Annealing Warm Restarts scheduler (epoch-based)\n",
        "        # T_mult (scheduler_T_mult) is now correctly passed, enabling warm restarts cycle length to increase\n",
        "        self.scheduler = optim.lr_scheduler.CosineAnnealingWarmRestarts(\n",
        "            self.optimizer,\n",
        "            T_0=scheduler_T_0,\n",
        "            T_mult=scheduler_T_mult,\n",
        "            eta_min=scheduler_eta_min\n",
        "        )\n",
        "\n",
        "        # Gradient scaler for mixed precision\n",
        "        self.scaler = GradScaler()\n",
        "        self.huber_loss = nn.SmoothL1Loss(beta=1.0)\n",
        "\n",
        "        # Branch/trunk scale (warmup is separate from scheduler T_0 now)\n",
        "        self.warmup_epochs = warmup_epochs\n",
        "        self.initial_scale = initial_scale\n",
        "        self.final_scale = final_scale\n",
        "        if hasattr(self.model, 'branch_scale') and hasattr(self.model, 'trunk_scale'):\n",
        "            with torch.no_grad():\n",
        "                self.model.branch_scale.fill_(initial_scale)\n",
        "                self.model.trunk_scale.fill_(initial_scale)\n",
        "\n",
        "    def check_model_health(self, epoch, batch_idx):\n",
        "        for name, param in self.model.named_parameters():\n",
        "            if torch.isnan(param).any() or torch.isinf(param).any():\n",
        "                print(f\"Bad parameter: {name} at Epoch {epoch}, Batch {batch_idx}\")\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def compute_loss(self, pred_cashflow, true_cashflow, pred_deriv=None, true_deriv=None, mask=None):\n",
        "\n",
        "        cashflow_loss = self.huber_loss(pred_cashflow, true_cashflow)\n",
        "\n",
        "        # --- Derivative loss (no masking needed) ---\n",
        "        if pred_deriv is not None and true_deriv is not None:\n",
        "            deriv_loss = self.huber_loss(pred_deriv, true_deriv)\n",
        "            total_loss = cashflow_loss + self.lambda_deriv_weight * deriv_loss\n",
        "        else:\n",
        "            deriv_loss = torch.tensor(0.0, device=pred_cashflow.device)\n",
        "            total_loss = cashflow_loss\n",
        "\n",
        "        return total_loss, cashflow_loss, deriv_loss\n",
        "\n",
        "    def train_step(self, portfolio, S_T, cashflow, true_derivative=None, mask=None, epoch=0, batch_idx=0, log_gradients=False):\n",
        "      self.optimizer.zero_grad()\n",
        "\n",
        "      S_T = S_T.clone().detach().requires_grad_(True).to(self.device)\n",
        "      portfolio = portfolio.to(self.device)\n",
        "      cashflow = cashflow.to(self.device)\n",
        "      if mask is not None:\n",
        "          mask = mask.to(self.device)\n",
        "      if true_derivative is not None:\n",
        "          true_derivative = true_derivative.to(self.device)\n",
        "\n",
        "      pred_cashflow = self.model(portfolio, S_T, mask=mask)\n",
        "\n",
        "      pred_deriv = None\n",
        "      if true_derivative is not None:\n",
        "          # FIXED: Compute gradients per scenario, not summed\n",
        "          pred_deriv = torch.autograd.grad(\n",
        "              outputs=pred_cashflow,\n",
        "              inputs=S_T,\n",
        "              grad_outputs=torch.ones_like(pred_cashflow),  # ADD THIS LINE\n",
        "              retain_graph=True,\n",
        "              create_graph=True,\n",
        "              allow_unused=True\n",
        "          )[0]\n",
        "\n",
        "      total_loss, cashflow_loss, deriv_loss = self.compute_loss(\n",
        "          pred_cashflow, cashflow, pred_deriv, true_derivative, mask=None\n",
        "      )\n",
        "\n",
        "      self.scaler.scale(total_loss).backward()\n",
        "      self.scaler.unscale_(self.optimizer)\n",
        "      torch.nn.utils.clip_grad_norm_(self.model.parameters(), max_norm=1.0)\n",
        "      self.scaler.step(self.optimizer)\n",
        "      self.scaler.update()\n",
        "\n",
        "      return total_loss.item(), cashflow_loss.item(), deriv_loss.item()\n",
        "\n",
        "    def val_step(self, portfolio, S_T, cashflow, true_derivative=None, mask=None):\n",
        "      \"\"\"\n",
        "      Validation step that can compute derivative loss as well.\n",
        "      \"\"\"\n",
        "      self.model.eval()\n",
        "\n",
        "      portfolio = portfolio.to(self.device)\n",
        "      S_T = S_T.clone().detach().requires_grad_(True).to(self.device)  # need grad for derivative\n",
        "      cashflow = cashflow.to(self.device)\n",
        "      if mask is not None:\n",
        "          mask = mask.to(self.device)\n",
        "      if true_derivative is not None:\n",
        "          true_derivative = true_derivative.to(self.device)\n",
        "\n",
        "      # always compute forward with grad enabled (so we can use for both losses)\n",
        "      with torch.enable_grad():\n",
        "          pred_cashflow = self.model(portfolio, S_T, mask=mask)\n",
        "\n",
        "          pred_deriv = None\n",
        "          if true_derivative is not None:\n",
        "              pred_deriv = torch.autograd.grad(\n",
        "                  outputs=pred_cashflow,\n",
        "                  inputs=S_T,\n",
        "                  grad_outputs=torch.ones_like(pred_cashflow),\n",
        "                  retain_graph=False,\n",
        "                  create_graph=False,\n",
        "                  allow_unused=True\n",
        "              )[0]\n",
        "\n",
        "      # detach before computing loss to avoid holding graph in memory\n",
        "      total_loss, cashflow_loss, deriv_loss = self.compute_loss(\n",
        "          pred_cashflow.detach(), cashflow,\n",
        "          pred_deriv.detach() if pred_deriv is not None else None,\n",
        "          true_derivative, mask=None\n",
        "      )\n",
        "\n",
        "      return total_loss.item(), cashflow_loss.item(), deriv_loss.item()\n",
        "\n",
        "    def step_scheduler_epoch(self):\n",
        "        \"\"\"Step scheduler once per epoch.\"\"\"\n",
        "        self.scheduler.step()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tV-kkl5-hHpX"
      },
      "source": [
        "# Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "mJHJlqHAhG8F"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "\n",
        "\n",
        "def get_stable_hyperparameters():\n",
        "\n",
        "    return {\n",
        "        \"learning_rate\": 3e-4,\n",
        "        \"weight_decay\": 1e-5,\n",
        "        \"lambda_deriv\": 1,\n",
        "        \"lambda_reg\": 1e-4,\n",
        "        \"gradient_clip_norm\": 1,\n",
        "        \"batch_size\": 128,\n",
        "        \"scheduler_T0\": 20,\n",
        "        \"scheduler_T_mult\":2,\n",
        "        \"scheduler_eta_min\":1e-6,\n",
        "        \"early_stopping_patience\": 50,\n",
        "    }\n",
        "\n",
        "HIDDEN_DIM = 128  # Increased capacity\n",
        "LATENT_DIM = 256 # Maintain\n",
        "BATCH_SIZE = 128\n",
        "TOTAL_EPOCHS = 500\n",
        "FEATURE_DIM = 3\n",
        "PORFOLIO_MAX_LENGTH = 500\n",
        "PORT_SAMPLE_SIZE = 51200\n",
        "FEED_ST_LEN_PER_PORT = 100\n",
        "\n",
        "CAPACITY_EVAL = 1000\n",
        "\n",
        "# === SAVE SCALERS ===\n",
        "DRIVE_PATH = \"/content/drive/MyDrive/15Oct/\"\n",
        "CHECK_POIN_DIR = DRIVE_PATH + \"checkpoints/\"\n",
        "os.makedirs(CHECK_POIN_DIR, exist_ok=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-GYP17eShEPK"
      },
      "source": [
        "# Save Check points function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "nJgCkcS0gzCE"
      },
      "outputs": [],
      "source": [
        "\n",
        "def save_model_checkpoint(model, save_path, epoch=None, optimizer=None, scheduler=None,\n",
        "                         train_loss=None, val_loss=None, train_size=None, val_size=None):\n",
        "\n",
        "    checkpoint_data = {\n",
        "        \"model_state_dict\": model.state_dict(),\n",
        "        \"hparams\": {\n",
        "            \"hidden_dim\": HIDDEN_DIM,\n",
        "            \"latent_dim\": LATENT_DIM,\n",
        "            \"portfolio_feature_dim\": FEATURE_DIM,\n",
        "            \"use_enhanced_transformer\": True\n",
        "        },\n",
        "        \"training_config\": {\n",
        "            \"PORT_LEN\": PORFOLIO_MAX_LENGTH,\n",
        "            \"PORT_SAMPLE_SIZE\": PORT_SAMPLE_SIZE,\n",
        "            \"FEED_ST_LEN_EACH_PORT\": FEED_ST_LEN_PER_PORT,\n",
        "            \"batch_size\": BATCH_SIZE,\n",
        "            \"train_size\": train_size,\n",
        "            \"val_size\": val_size\n",
        "        },\n",
        "        \"scaler_files\": {\n",
        "            \"K_scaler\": \"K_Scalar_Training.pkl\",\n",
        "            \"S_T_scaler\": \"S_T_Scalar_Training.pkl\",\n",
        "            \"cashflow_scaler\": \"Cashflow_Scalar_Training.pkl\"\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Add optional training state information\n",
        "    if epoch is not None:\n",
        "        checkpoint_data[\"epoch\"] = epoch\n",
        "    if optimizer is not None:\n",
        "        checkpoint_data[\"optimizer_state_dict\"] = optimizer.state_dict()\n",
        "    if scheduler is not None:\n",
        "        checkpoint_data[\"scheduler_state_dict\"] = scheduler.state_dict()\n",
        "    if train_loss is not None:\n",
        "        checkpoint_data[\"train_loss\"] = train_loss\n",
        "    if val_loss is not None:\n",
        "        checkpoint_data[\"val_loss\"] = val_loss\n",
        "\n",
        "    torch.save(checkpoint_data, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "EAEXt7DMjPhc"
      },
      "outputs": [],
      "source": [
        "def fit_scalars_and_save():\n",
        "  raw_dataset = DatasetStandardized(\n",
        "        num_samples=PORT_SAMPLE_SIZE,\n",
        "        min_portfolio_size=1,\n",
        "        max_portfolio_size=CAPACITY_EVAL,\n",
        "        num_samples_S_T=FEED_ST_LEN_PER_PORT,\n",
        "        is_fitting_mode=True\n",
        "    )\n",
        "  raw_loader_fitting = DataLoader(\n",
        "        raw_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn)\n",
        "\n",
        "  print(f\"Fitting scalers on RAW data of size {CAPACITY_EVAL}\")\n",
        "  K_scalar, S_T_scalar = fit_K_ST_scalers(raw_loader_fitting)\n",
        "  cashflow_scaler = fit_cashflow_scaler(raw_loader_fitting)\n",
        "\n",
        "  try:\n",
        "      joblib.dump(K_scalar, DRIVE_PATH + \"K_Scalar_Training.pkl\")\n",
        "      joblib.dump(S_T_scalar, DRIVE_PATH + \"S_T_Scalar_Training.pkl\")\n",
        "      joblib.dump(cashflow_scaler, DRIVE_PATH + \"Cashflow_Scalar_Training.pkl\")\n",
        "      print(\"Successfully saved all training scalers\")\n",
        "\n",
        "      print(f\"Scaler Statistics:\")\n",
        "      print(f\"K_scaler - mean: {K_scalar.mean_[0]:.4f}, std: {np.sqrt(K_scalar.var_[0]):.4f}\")\n",
        "      print(f\"S_T_scaler - mean: {S_T_scalar.mean_[0]:.4f}, std: {np.sqrt(S_T_scalar.var_[0]):.4f}\")\n",
        "      print(f\"Cashflow_scaler - mean: {cashflow_scaler.mean_[0]:.4f}, std: {np.sqrt(cashflow_scaler.var_[0]):.4f}\")\n",
        "      return K_scalar, S_T_scalar, cashflow_scaler\n",
        "  except Exception as e:\n",
        "      print(f\"Error saving scalers: {e}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k7uSWj-uhCQC",
        "outputId": "113a9375-5b34-4310-c50c-0ae878f6ae48"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m An experiment with the same configuration options is already running and will be reused.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fitting scalers on RAW data of size 1000\n",
            "Fitting K and S_T scalers from training set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Collecting K and S_T for scalers:  10%|█         | 40/400 [00:13<02:02,  2.94it/s]"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "import numpy as np\n",
        "import joblib\n",
        "from comet_ml import start\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "\n",
        "# === COMET SETUP ===\n",
        "experiment = start(\n",
        "    api_key=\"iatWnXT4JyBtDQhn7OfgISQoF\",\n",
        "    project_name=\"option-portfolio-encoder-decoder\",\n",
        "    workspace=\"satyabratkumarsingh\"\n",
        ")\n",
        "\n",
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "SEED = 42\n",
        "torch.manual_seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "random.seed(SEED)\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Training loop with curriculum learning on portfolio sizes.\n",
        "    \"\"\"\n",
        "    hparams = get_stable_hyperparameters()\n",
        "    experiment.log_parameters(hparams)\n",
        "\n",
        "    # === Model ===\n",
        "    model = DeepONet(\n",
        "        portfolio_feature_dim=FEATURE_DIM,\n",
        "        hidden_dim=HIDDEN_DIM,\n",
        "        latent_dim=LATENT_DIM,\n",
        "        use_pma=True,\n",
        "        num_seeds=NUM_SEEDS\n",
        "    ).to(DEVICE)\n",
        "\n",
        "    K_scalar, S_T_scalar, cashflow_scaler = fit_scalars_and_save()\n",
        "\n",
        "    # Now get the training and validation dataset\n",
        "    normalized_dataset = DatasetStandardized(\n",
        "        num_samples=PORT_SAMPLE_SIZE,\n",
        "        min_portfolio_size=1,\n",
        "        max_portfolio_size=PORFOLIO_MAX_LENGTH,\n",
        "        num_samples_S_T=FEED_ST_LEN_PER_PORT,\n",
        "        K_scaler=K_scalar,\n",
        "        S_T_scaler=S_T_scalar,\n",
        "        cashflow_scaler=cashflow_scaler,\n",
        "        is_fitting_mode=False\n",
        "    )\n",
        "    train_size = int(0.8 * len(normalized_dataset))\n",
        "    val_size = len(normalized_dataset) - train_size\n",
        "    val_size = (val_size // BATCH_SIZE) * BATCH_SIZE\n",
        "    train_size = len(normalized_dataset) - val_size\n",
        "\n",
        "\n",
        "    train_dataset, val_dataset = random_split(normalized_dataset, [train_size, val_size])\n",
        "    train_loader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "    val_loader = DataLoader(\n",
        "        val_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        collate_fn=collate_fn\n",
        "    )\n",
        "\n",
        "    # === Trainer ===\n",
        "    trainer = OptimizedTrainer(\n",
        "    model,\n",
        "    device=DEVICE,\n",
        "    learning_rate=hparams[\"learning_rate\"],\n",
        "    lambda_deriv_weight=hparams[\"lambda_deriv\"],\n",
        "    weight_decay=hparams[\"weight_decay\"],\n",
        "    monitor_gradients=True,\n",
        "    grad_log_threshold=5.0,\n",
        "    scheduler_T_0=hparams[\"scheduler_T0\"],\n",
        "    scheduler_T_mult=hparams[\"scheduler_T_mult\"],\n",
        "    scheduler_eta_min=hparams[\"scheduler_eta_min\"],\n",
        "    warmup_epochs=5,\n",
        "    initial_scale=0.05,\n",
        "    final_scale=1.0\n",
        "    )\n",
        "\n",
        "\n",
        "    early_stopper = ExtendedEarlyStopping(\n",
        "        patience=50, min_delta=0.001, restore_best_weights=True\n",
        "    )\n",
        "\n",
        "    # === Track best model ===\n",
        "    best_val_loss = float('inf')\n",
        "    best_epoch = 0\n",
        "\n",
        "    for epoch in range(TOTAL_EPOCHS):\n",
        "\n",
        "        train_dataset.dataset.set_epoch(epoch)\n",
        "        model.train()\n",
        "\n",
        "        # --- Training Loop ---\n",
        "        train_total_losses, train_cf_losses, train_deriv_losses = [], [], []\n",
        "\n",
        "        for batch_idx, batch in enumerate(tqdm(train_loader, desc=f\"Training Epoch {epoch}\")):\n",
        "\n",
        "            portfolio = batch[\"portfolio\"].to(DEVICE)\n",
        "            portfolio = portfolio[:, :, :FEATURE_DIM]\n",
        "\n",
        "            mask = batch[\"mask\"].to(DEVICE)\n",
        "            S_T = batch[\"S_T\"].to(DEVICE)\n",
        "            cashflow = batch[\"cashflow\"].to(DEVICE)\n",
        "            derivative = batch[\"derivative\"].to(DEVICE)\n",
        "\n",
        "            # Training step\n",
        "            total, cf, deriv = trainer.train_step(\n",
        "                portfolio,\n",
        "                S_T.clone().detach().requires_grad_(True),\n",
        "                cashflow,\n",
        "                derivative,\n",
        "                mask\n",
        "            )\n",
        "\n",
        "            train_total_losses.append(total)\n",
        "            train_cf_losses.append(cf)\n",
        "            train_deriv_losses.append(deriv)\n",
        "\n",
        "            # Batch-level logging\n",
        "            global_step = epoch * len(train_loader) + batch_idx\n",
        "            experiment.log_metric(\"train_total_loss_batch\", total, step=global_step)\n",
        "            experiment.log_metric(\"train_cashflow_loss_batch\", cf, step=global_step)\n",
        "            experiment.log_metric(\"train_derivative_loss_batch\", deriv, step=global_step)\n",
        "\n",
        "        # Epoch-level training metrics\n",
        "        avg_train_total = np.mean(train_total_losses)\n",
        "        avg_train_cf = np.mean(train_cf_losses)\n",
        "        avg_train_deriv = np.mean(train_deriv_losses)\n",
        "\n",
        "        experiment.log_metric(\"train_total_loss_epoch\", avg_train_total, step=epoch)\n",
        "        experiment.log_metric(\"train_cashflow_loss_epoch\", avg_train_cf, step=epoch)\n",
        "        experiment.log_metric(\"train_derivative_loss_epoch\", avg_train_deriv, step=epoch)\n",
        "\n",
        "\n",
        "        model.eval()\n",
        "        val_total_losses, val_cf_losses, val_deriv_losses = [], [], []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            for batch_idx, batch in enumerate(val_loader):\n",
        "                portfolio = batch[\"portfolio\"].to(DEVICE)\n",
        "                portfolio = portfolio[:, :, :FEATURE_DIM]\n",
        "\n",
        "                mask = batch[\"mask\"].to(DEVICE)\n",
        "                S_T = batch[\"S_T\"].to(DEVICE)\n",
        "                cashflow = batch[\"cashflow\"].to(DEVICE)\n",
        "                derivative = batch[\"derivative\"].to(DEVICE)\n",
        "\n",
        "                # Validation step\n",
        "                total, cf, deriv = trainer.val_step(\n",
        "                    portfolio, S_T, cashflow, derivative, mask\n",
        "                )\n",
        "\n",
        "                val_total_losses.append(total)\n",
        "                val_cf_losses.append(cf)\n",
        "                val_deriv_losses.append(deriv)\n",
        "\n",
        "                # Batch-level validation logging\n",
        "                global_step = epoch * len(val_loader) + batch_idx\n",
        "                experiment.log_metric(\"val_total_loss_batch\", total, step=global_step)\n",
        "                experiment.log_metric(\"val_cashflow_loss_batch\", cf, step=global_step)\n",
        "                experiment.log_metric(\"val_derivative_loss_batch\", deriv, step=global_step)\n",
        "\n",
        "        # Step the scheduler\n",
        "        trainer.step_scheduler_epoch()\n",
        "\n",
        "        current_lr = trainer.optimizer.param_groups[0]['lr']\n",
        "\n",
        "        # Epoch-level validation metrics\n",
        "        avg_val_total = np.mean(val_total_losses)\n",
        "        avg_val_cf = np.mean(val_cf_losses)\n",
        "        avg_val_deriv = np.mean(val_deriv_losses)\n",
        "\n",
        "        experiment.log_metric(\"val_total_loss_epoch\", avg_val_total, step=epoch)\n",
        "        experiment.log_metric(\"val_cashflow_loss_epoch\", avg_val_cf, step=epoch)\n",
        "        experiment.log_metric(\"val_derivative_loss_epoch\", avg_val_deriv, step=epoch)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch [{epoch}/{TOTAL_EPOCHS}] \"\n",
        "            f\"Train → total: {avg_train_total:.6f}, cf: {avg_train_cf:.6f}, deriv: {avg_train_deriv:.6f} | \"\n",
        "            f\"Val → total: {avg_val_total:.6f}, cf: {avg_val_cf:.6f}, deriv: {avg_val_deriv:.6f}\"\n",
        "        )\n",
        "\n",
        "        # === SAVE BEST MODEL ===\n",
        "        if avg_val_total < best_val_loss:\n",
        "            best_val_loss = avg_val_total\n",
        "            best_epoch = epoch + 1\n",
        "\n",
        "            best_checkpoint_path = CHECK_POIN_DIR + \"deeponet_model-BEST.pt\"\n",
        "            save_model_checkpoint(\n",
        "                model=model,\n",
        "                save_path=best_checkpoint_path,\n",
        "                epoch=epoch + 1,\n",
        "                optimizer=trainer.optimizer,\n",
        "                scheduler=trainer.scheduler if hasattr(trainer, 'scheduler') else None,\n",
        "                train_loss=avg_train_total,\n",
        "                val_loss=avg_val_total\n",
        "            )\n",
        "            print(f\"NEW BEST MODEL! Epoch {epoch + 1}, Val Loss: {avg_val_total:.6f}\")\n",
        "\n",
        "        # === SAVE MODEL EVERY 50 EPOCHS ===\n",
        "        if (epoch + 1) % 50 == 0:\n",
        "            checkpoint_path = CHECK_POIN_DIR + f\"deeponet_model-epoch{epoch + 1}.pt\"\n",
        "            save_model_checkpoint(\n",
        "                model=model,\n",
        "                save_path=checkpoint_path,\n",
        "                epoch=epoch + 1,\n",
        "                optimizer=trainer.optimizer,\n",
        "                scheduler=trainer.scheduler if hasattr(trainer, 'scheduler') else None,\n",
        "                train_loss=avg_train_total,\n",
        "                val_loss=avg_val_total\n",
        "            )\n",
        "            print(f\"Model checkpoint saved at epoch {epoch + 1}: {checkpoint_path}\")\n",
        "\n",
        "        # --- Early Stopping Check ---\n",
        "        stop = early_stopper(avg_val_total, model)\n",
        "        if stop:\n",
        "            print(f\"Early stopping triggered at epoch {epoch}. Best val loss: {early_stopper.best:.6f}\")\n",
        "            break\n",
        "\n",
        "    # === SAVE FINAL MODEL ===\n",
        "    save_path = DRIVE_PATH + \"final_deeponet_model.pt\"\n",
        "    save_model_checkpoint(model=model, save_path=save_path)\n",
        "\n",
        "    # === TRAINING SUMMARY ===\n",
        "    print(\"\\n\" + \"=\"*70)\n",
        "    print(\"TRAINING COMPLETE\")\n",
        "    print(\"=\"*70)\n",
        "    print(f\"Best Model:\")\n",
        "    print(f\"  - Epoch: {best_epoch}\")\n",
        "    print(f\"  - Validation Loss: {best_val_loss:.6f}\")\n",
        "    print(f\"  - Saved at: {CHECK_POIN_DIR}deeponet_model-BEST.pt\")\n",
        "    print(f\"\\nFinal Model:\")\n",
        "    print(f\"  - Saved at: {save_path}\")\n",
        "    print(f\"\\nCheckpoints:\")\n",
        "    print(f\"  - Directory: {CHECK_POIN_DIR}\")\n",
        "    print(f\"  - Saved every 50 epochs\")\n",
        "    print(f\"\\nScalers:\")\n",
        "    print(f\"  - Saved at: {DRIVE_PATH}*_Scalar_Training.pkl\")\n",
        "    print(\"=\"*70)\n",
        "\n",
        "    experiment.end()\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M4qwIFeSmqP-",
        "outputId": "09fa9bdd-482d-4dab-956e-18210fb32736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : okay_rosin_3999\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/satyabratkumarsingh/option-portfolio-encoder-decoder/fa8bd5ba49bb4137981ef05361e45c09\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_cashflow_loss_batch [30400]   : (0.01226563099771738, 0.147788405418396)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_cashflow_loss_epoch [95]      : (0.022968936304096133, 0.08902193123940379)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_derivative_loss_batch [30400] : (0.022049417719244957, 0.3778359591960907)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_derivative_loss_epoch [95]    : (0.0661961613339372, 0.1701654148520902)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_total_loss_batch [30400]      : (0.037341564893722534, 0.4871252179145813)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_total_loss_epoch [95]         : (0.08986473205732182, 0.25918734571896496)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_cashflow_loss_batch [7574]      : (0.027661651372909546, 0.1530752182006836)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_cashflow_loss_epoch [94]        : (0.06575393648818136, 0.0865249362308532)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_derivative_loss_batch [7574]    : (0.04674411565065384, 0.36890533566474915)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_derivative_loss_epoch [94]      : (0.11872582798823714, 0.16324777202680707)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_total_loss_batch [7574]         : (0.07952849566936493, 0.46901971101760864)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_total_loss_epoch [94]           : (0.18475438943132758, 0.24977270811796187)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook_url : https://colab.research.google.com/notebook#fileId=1L8tZjZp5hPpQN2po0UXt8NyXSlNAUseD\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size              : 128\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     curriculum_learning     : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     early_stopping_patience : 50\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gradient_clip_norm      : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lambda_deriv            : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lambda_reg              : 0.0001\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate           : 0.0002\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     scheduler_T0            : 20\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     weight_decay            : 1e-05\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 2\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n"
          ]
        }
      ],
      "source": [
        "experiment.end()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YPQZwBoEn2e7",
        "outputId": "cfc4b946-af03-44e8-932f-8ed1eabd52a3"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m Comet.ml Experiment Summary\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m ---------------------------------------------------------------------------------------\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Data:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     display_summary_level : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     name                  : balanced_dowel_6941\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     url                   : https://www.comet.com/satyabratkumarsingh/option-portfolio-encoder-decoder/99a57b0cd3da4bb4bd9e4a2180650536\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Metrics [count] (min, max):\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_cashflow_loss_batch [79205]   : (0.006972298491746187, 0.1393452286720276)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_cashflow_loss_epoch [247]     : (0.01495979430328589, 0.08376031016232446)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_derivative_loss_batch [79205] : (0.01581203006207943, 0.3616739511489868)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_derivative_loss_epoch [247]   : (0.037467062700306995, 0.1609659396111965)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_total_loss_batch [79205]      : (0.024923114106059074, 0.45853346586227417)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     train_total_loss_epoch [247]        : (0.05250067715533078, 0.24472624962218106)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_cashflow_loss_batch [19760]     : (0.006313389167189598, 0.10672395676374435)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_cashflow_loss_epoch [247]       : (0.012287375645246356, 0.06236603423021734)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_derivative_loss_batch [19760]   : (0.014233853667974472, 0.25698962807655334)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_derivative_loss_epoch [247]     : (0.02942046062089503, 0.1304710996337235)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_total_loss_batch [19760]        : (0.0225725919008255, 0.3637135922908783)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     val_total_loss_epoch [247]          : (0.04207148705609143, 0.19283713381737472)\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Others:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook_url : https://colab.research.google.com/notebook#fileId=1L8tZjZp5hPpQN2po0UXt8NyXSlNAUseD\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Parameters:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     batch_size              : 128\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     curriculum_learning     : True\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     early_stopping_patience : 50\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     gradient_clip_norm      : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lambda_deriv            : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     lambda_reg              : 0.0001\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     learning_rate           : 0.0002\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     scheduler_T0            : 20\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     weight_decay            : 1e-05\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m   Uploads:\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     environment details : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     filename            : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     installed packages  : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     notebook            : 2\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     os packages         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m     source_code         : 1\n",
            "\u001b[1;38;5;39mCOMET INFO:\u001b[0m \n",
            "\u001b[1;38;5;214mCOMET WARNING:\u001b[0m To get all data logged automatically, import comet_ml before the following modules: sklearn, torch.\n"
          ]
        }
      ],
      "source": [
        "experiment.end()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "L4",
      "machine_shape": "hm",
      "provenance": [],
      "authorship_tag": "ABX9TyOgEzrv1u29CeefBnRtvpiL",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}